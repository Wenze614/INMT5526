warning: LF will be replaced by CRLF in project-2/.Rproj.user/100C845F/sources/prop/DC1EA05B.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in project-2/.Rproj.user/100C845F/sources/prop/INDEX.
The file will have its original line endings in your working directory
warning: LF will be replaced by CRLF in project-2/project.R.
The file will have its original line endings in your working directory
[1mdiff --git a/project-2/.Rproj.user/100C845F/sources/prop/DC1EA05B b/project-2/.Rproj.user/100C845F/sources/prop/DC1EA05B[m
[1mindex 7fa0647..dd68951 100644[m
[1m--- a/project-2/.Rproj.user/100C845F/sources/prop/DC1EA05B[m
[1m+++ b/project-2/.Rproj.user/100C845F/sources/prop/DC1EA05B[m
[36m@@ -2,5 +2,5 @@[m
     "source_window_id": "",[m
     "Source": "Source",[m
     "cursorPosition": "62,0",[m
[31m-    "scrollLine": "56"[m
[32m+[m[32m    "scrollLine": "0"[m
 }[m
\ No newline at end of file[m
[1mdiff --git a/project-2/.Rproj.user/100C845F/sources/prop/INDEX b/project-2/.Rproj.user/100C845F/sources/prop/INDEX[m
[1mindex 075f6f8..1141a15 100644[m
[1m--- a/project-2/.Rproj.user/100C845F/sources/prop/INDEX[m
[1m+++ b/project-2/.Rproj.user/100C845F/sources/prop/INDEX[m
[36m@@ -1,3 +1,4 @@[m
 C%3A%2Funi%2FINMT5526%2FDecisionTree%2FDecisionTree%2FScript%2Fdecisiontree.R="9D31F0F8"[m
[32m+[m[32mC%3A%2Funi%2FINMT5526%2FINMT5526%2Fproject-2%2Fproject.R="7AC7A156"[m
 C%3A%2Funi%2FINMT5526%2FWeekEleven.r="DC1EA05B"[m
 C%3A%2Funi%2FINMT5526%2Fproject-2%2Fproject-2%2Fproject.R="D395C1B8"[m
[1mdiff --git a/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/0735D7DE b/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/0735D7DE[m
[1mdeleted file mode 100644[m
[1mindex 4e2410a..0000000[m
[1m--- a/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/0735D7DE[m
[1m+++ /dev/null[m
[36m@@ -1,26 +0,0 @@[m
[31m-{[m
[31m-    "id": "0735D7DE",[m
[31m-    "path": "C:/uni/INMT5526/WeekEleven.r",[m
[31m-    "project_path": null,[m
[31m-    "type": "r_source",[m
[31m-    "hash": "0",[m
[31m-    "contents": "",[m
[31m-    "dirty": false,[m
[31m-    "created": 1633936877886.0,[m
[31m-    "source_on_save": false,[m
[31m-    "relative_order": 5,[m
[31m-    "properties": {[m
[31m-        "source_window_id": "",[m
[31m-        "Source": "Source",[m
[31m-        "cursorPosition": "62,0",[m
[31m-        "scrollLine": "56"[m
[31m-    },[m
[31m-    "folds": "",[m
[31m-    "lastKnownWriteTime": 1633936877,[m
[31m-    "encoding": "UTF-8",[m
[31m-    "collab_server": "",[m
[31m-    "source_window": "",[m
[31m-    "last_content_update": 1633936877,[m
[31m-    "read_only": false,[m
[31m-    "read_only_alternatives": [][m
[31m-}[m
\ No newline at end of file[m
[1mdiff --git a/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/0735D7DE-contents b/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/0735D7DE-contents[m
[1mdeleted file mode 100644[m
[1mindex 1f2826e..0000000[m
[1m--- a/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/0735D7DE-contents[m
[1m+++ /dev/null[m
[36m@@ -1,191 +0,0 @@[m
[31m-#######################################################[m
[31m-# Week Eleven: Hyperparameters and Tuning the Network #[m
[31m-# Business Intelligence (INMT5526)                    #[m
[31m-#######################################################[m
[31m-[m
[31m-# This week, we will be generating a regression neural network model as opposed [m
[31m-# to a classification one, for experience with something different. From here, [m
[31m-# we will use caret to undertake hyperparameter tuning, which simply means [m
[31m-# trying different parameter values to see what generates the most useful [m
[31m-# network. Finally, we will undertake cross validation as another method to [m
[31m-# improve our network; this is similar to bagging with decision trees.[m
[31m-[m
[31m-# Firstly, if we so desire, let us clean up our workspace.[m
[31m-rm(list = ls())[m
[31m-[m
[31m-# Next, we must import the packages that we will need to undertake our [m
[31m-# work with ANN's. These will be the same as last week as we still need [m
[31m-# 'neuralnet' for some utilities. We add the 'MASS' package for this dataset [m
[31m-# which will be described below.[m
[31m-if (!require("pacman")) install.packages("pacman")[m
[31m-pacman::p_load("dplyr", "caret", "nnet", "neuralnet", "NeuralNetTools", "MASS")[m
[31m-[m
[31m-##########################################################[m
[31m-# PART ONE: Basic regression Neural Network with 'nnet'. #[m
[31m-##########################################################[m
[31m-[m
[31m-# It is safe to say from our experiences last week, 'nnet' is easier to use [m
[31m-# than 'neuralnet', however we can use either. This week, we will be looking [m
[31m-# at a regression problem - house prices in Boston using the 'MASS' dataset [m
[31m-# built in to R. We will load this in the same way we did with the iris dataset,[m
[31m-# and then explore it.[m
[31m-house_df <- Boston[m
[31m-summary(house_df)[m
[31m-[m
[31m-# For more information on each of the columns, read the R Documentation at the [m
[31m-# following address: https://www.rdocumentation.org/packages/MASS/versions/[m
[31m-# 7.3-54/topics/Boston. Yes, that median value really is in thousands![m
[31m-[m
[31m-# Do any changes need to be made to the datatypes within this dataset? If so, [m
[31m-# make them below this comment. (HINT: there is one, recall using as.factor).[m
[31m-[m
[31m-# I disagree with the racial component being included in this dataset, so let's [m
[31m-# remove it now using 'dplyr'.[m
[31m-house_df <- house_df %>% dplyr::select(-black)[m
[31m-[m
[31m-# It is a good idea to normalise our data before training the neural network. [m
[31m-# Depending on the dataset, this could save us a bunch of time and/or the [m
[31m-# ability to generate meaningful results at all - the model may not converge. [m
[31m-# We will scale between [0, 1] using the code below.[m
[31m-normalise_func <- function(x) { (x - min(x)) / (max(x) - min(x))}[m
[31m-house_scaled_df <- house_df %>% mutate(across(where(is.numeric), [m
[31m-  normalise_func))[m
[31m-[m
[31m-# We then need to split our dataset, as we almost always do with our machine [m
[31m-# learning algorithms, into our training and test datasets. This remains [m
[31m-# unchanged from our use of 'createDataPartition'. We want to use our 'medv' [m
[31m-# (median house value) instead.[m
[31m-train_idx <- createDataPartition(house_scaled_df$medv, p = 0.8, list = FALSE)[m
[31m-house_train_df <- house_scaled_df[train_idx, ][m
[31m-house_test_df <- house_scaled_df[-train_idx, ][m
[31m-[m
[31m-# We can now run our model using 'caret' and 'nnet'.[m
[31m-house_net <- train(medv ~ ., house_train_df, method = 'nnet', linout = TRUE, [m
[31m-  trace = FALSE)[m
[31m-[m
[31m-# Let us look at some information from our training. Your tutor will explain [m
[31m-# what you are seeing.[m
[31m-print(house_net)[m
[31m-[m
[31m-# We can then predict over the test set, as we did before.[m
[31m-house_pred <- predict(house_net, house_test_df)[m
[31m-[m
[31m-# Our results must differ due to the fact we are predicting something [m
[31m-# continuous. Hence, we can look at a standard plot of predicted vs actual. But[m
[31m-# first, we need to generate a DataFrame showing this.[m
[31m-house_pa_df <- cbind(as.data.frame(house_pred), house_test_df$medv)[m
[31m-colnames(house_pa_df) <- c("Predicted", "Actual")[m
[31m-[m
[31m-# Then, let us plot an actual graph of the predictions (with a 45 degree line [m
[31m-# that indicates prediction = actual).[m
[31m-plot(x = house_pa_df$Predicted, y = house_pa_df$Actual) + abline(0, 1)[m
[31m-[m
[31m-# We can still look at the actual plot of the neural network.[m
[31m-plotnet(house_net)[m
[31m-[m
[31m-##########################################################[m
[31m-# PART TWO: Hyperparameter tuning with 'Boston' dataset. #[m
[31m-##########################################################[m
[31m-[m
[31m-# The above model seems to do a decent job, however we may be able to get a [m
[31m-# bettter result with a different number of hidden neurons (after all, [m
[31m-# everyone disagrees on a good number) and/or number of hidden layers (likely [m
[31m-# not) as well as the decay function used as part of learning with [m
[31m-# backpropagation. We can adjust these using Caret. We can see above this was [m
[31m-# done automatically for us in the 'nnet' command. However, it is a bit more [m
[31m-# complex if we are doing it ourselves using a 'grid' of these option [m
[31m-# combinations. We can define a tree as follows, where each array defines the [m
[31m-# possible element values.[m
[31m-train_grid <- expand.grid(size = c(1, 3, 5, 7, 9), decay = c(0, 1e-4, 1e-3, [m
[31m-  1e-2, 1e-1))[m
[31m-[m
[31m-# We can then feed this into our 'train' command to use these as our parameter [m
[31m-# options, generating a network for each and then selecting the network with [m
[31m-# the smallest root mean square error (RMSE).[m
[31m-house_tune_net <- train(medv ~ ., house_train_df, method = 'nnet', [m
[31m-  linout = TRUE, trace = FALSE, tuneGrid = train_grid, metric = "RMSE",[m
[31m-  maxit = 100)[m
[31m-[m
[31m-# Let us see the results of doing this. Does this explain the error message [m
[31m-# you saw above? Change 'trace' to true, and it may reveal itself. Change [m
[31m-# 'maxit' to 9000 to see what this does as well. This will take a while, but [m
[31m-# should only affect those networks that need the extra iterations to converge [m
[31m-# (hint hint)![m
[31m-print(house_tune_net)[m
[31m-[m
[31m-# Depending on the random starting points, you may get a different answer, [m
[31m-# however the answers favour mid to larger values for both parameters (generally[m
[31m-# a decay of 0.01 and five or more neurons). Let us predict and plot as above.[m
[31m-house_tune_pred <- predict(house_tune_net, house_test_df)[m
[31m-house_tune_pa_df <- cbind(as.data.frame(house_tune_pred), house_test_df$medv)[m
[31m-colnames(house_tune_pa_df) <- c("Predicted", "Actual")[m
[31m-plot(x = house_tune_pa_df$Predicted, y = house_tune_pa_df$Actual) + abline(0, 1)[m
[31m-[m
[31m-# Let us look at the neural network itself. Was it really worth it? Compare the [m
[31m-# RMSE values for the chosen networks as well as the visuals.[m
[31m-plotnet(house_tune_net)[m
[31m-[m
[31m-# Fun fact, you can also do decision trees with 'caret' and 'train'. Do you [m
[31m-# recall similarities from earlier in semester?[m
[31m-[m
[31m-#######################################################[m
[31m-# PART THREE: Cross-validation with 'Boston' dataset. #[m
[31m-#######################################################[m
[31m-[m
[31m-# We can now look at using cross-validation. This is similar to the process of [m
[31m-# bagging that we use with decision trees. However, it is not really helpful [m
[31m-# except as an additional component to hyperparameter tuning. Hence, you will [m
[31m-# see here that the (default) hyperparameter tuning process is still undertaken.[m
[31m-house_cv_net <- train(medv ~., house_train_df, method = 'nnet', linout = TRUE, [m
[31m- trace = TRUE, metric = "RMSE", trControl = trainControl(method = "cv"))[m
[31m-[m
[31m-# Let us look at the result. Note the resampling section with cross-validation.[m
[31m-# Ten-fold simply means that the training data was split into ten and then [m
[31m-# the model fitted with one group as a test set and the other nine as the [m
[31m-# training sets, ten times.[m
[31m-print(house_cv_net)[m
[31m-[m
[31m-# Let us cheat and 'fix' our parameters to see if it offers any improvement [m
[31m-# in our diagnostic error characteristics compared to our generally 'best' [m
[31m-# outcome of tuning to five neurons and a decay of 0.01 above.[m
[31m-train_grid_fixed <- expand.grid(size = c(5), decay = c(0.01))[m
[31m-house_cv_net_fixed <- train(medv ~., house_train_df, method = 'nnet', [m
[31m-  linout = TRUE, trace = FALSE, metric = "RMSE", [m
[31m-  trControl = trainControl(method = "cv"), tuneGrid = train_grid_fixed)[m
[31m-print(house_cv_net_fixed)[m
[31m-[m
[31m-# For comparison, without the CV...[m
[31m-house_cv_net_fixed_nocv <- train(medv ~., house_train_df, method = 'nnet', [m
[31m-  linout = TRUE, trace = FALSE, metric = "RMSE", tuneGrid = train_grid_fixed)[m
[31m-print(house_cv_net_fixed_nocv)[m
[31m-[m
[31m-# If you wish, predict and test on the various models below.[m
[31m-[m
[31m-#########################################################################[m
[31m-# PART FOUR: Hyperparameter tuning and cross-validation with bank churn #[m
[31m-#            dataset.                                                   #[m
[31m-#########################################################################[m
[31m-[m
[31m-# This is where you come in! Replicate the process above but using the bank[m
[31m-# churn dataset that we used last week (the _ANN.csv version). Compare the [m
[31m-# results from your differing models. What are the best parameters?[m
[31m-[m
[31m-## PUT YOUR CODE HERE ##[m
[31m-[m
[31m-########################[m
[31m-# DISCUSSION QUESTIONS #[m
[31m-########################[m
[31m-[m
[31m-# There are no separate discussion questions in this practical. Rather, they are[m
[31m-# dispersed throughout the entirety of the practical above.[m
[31m-[m
[31m-# If you have any time left over, you may wish to converse with your team [m
[31m-# regarding the team assignment.[m
[31m-[m
[31m-# Next week, we will be looking at revision and blockchains. The last thing the [m
[31m-# world needs is a blockchain without a purpose, so the primary focus will be [m
[31m-# revision.[m
[31m-[m
[31m-############################[m
[31m-# END OF SEMINAR/PRACTICAL #[m
[31m-############################[m
\ No newline at end of file[m
[1mdiff --git a/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/37B82A47-contents b/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/37B82A47-contents[m
[1mdeleted file mode 100644[m
[1mindex e69de29..0000000[m
[1mdiff --git a/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/826D2B89-contents b/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/826D2B89-contents[m
[1mdeleted file mode 100644[m
[1mindex e69de29..0000000[m
[1mdiff --git a/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/9AF48C47-contents b/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/9AF48C47-contents[m
[1mdeleted file mode 100644[m
[1mindex e69de29..0000000[m
[1mdiff --git a/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/A32498B7 b/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/A32498B7[m
[1mdeleted file mode 100644[m
[1mindex fb56358..0000000[m
[1m--- a/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/A32498B7[m
[1m+++ /dev/null[m
[36m@@ -1,26 +0,0 @@[m
[31m-{[m
[31m-    "id": "A32498B7",[m
[31m-    "path": "C:/uni/INMT5526/project-2/project-2/project.R",[m
[31m-    "project_path": "project.R",[m
[31m-    "type": "r_source",[m
[31m-    "hash": "0",[m
[31m-    "contents": "",[m
[31m-    "dirty": true,[m
[31m-    "created": 1633329523211.0,[m
[31m-    "source_on_save": false,[m
[31m-    "relative_order": 1,[m
[31m-    "properties": {[m
[31m-        "source_window_id": "",[m
[31m-        "Source": "Source",[m
[31m-        "cursorPosition": "71,0",[m
[31m-        "scrollLine": "62"[m
[31m-    },[m
[31m-    "folds": "",[m
[31m-    "lastKnownWriteTime": 1633329456,[m
[31m-    "encoding": "UTF-8",[m
[31m-    "collab_server": "",[m
[31m-    "source_window": "",[m
[31m-    "last_content_update": 1633938196960,[m
[31m-    "read_only": false,[m
[31m-    "read_only_alternatives": [][m
[31m-}[m
\ No newline at end of file[m
[1mdiff --git a/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/A32498B7-contents b/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/A32498B7-contents[m
[1mdeleted file mode 100644[m
[1mindex 66caf10..0000000[m
[1m--- a/project-2/.Rproj.user/100C845F/sources/s-C9D3AB0D/A32498B7-contents[m
[1m+++ /dev/null[m
[36m@@ -1,150 +0,0 @@[m
[31m-audi = read.csv("./audi.csv")[m
[31m-install.packages('randomForest')[m
[31m-install.packages('pacman')[m
[31m-install.packages('gsubfn')[m
[31m-pacman::p_load(pacman, tidyverse, gmodels,ROCR, rpart, rpart.plot,caret)[m
[31m-library(randomForest)[m
[31m-library(gsubfn)[m
[31m-summary(audi)[m
[31m-[m
[31m-aud