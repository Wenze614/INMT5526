#######################################################
# Week Eleven: Hyperparameters and Tuning the Network #
# Business Intelligence (INMT5526)                    #
#######################################################

# This week, we will be generating a regression neural network model as opposed 
# to a classification one, for experience with something different. From here, 
# we will use caret to undertake hyperparameter tuning, which simply means 
# trying different parameter values to see what generates the most useful 
# network. Finally, we will undertake cross validation as another method to 
# improve our network; this is similar to bagging with decision trees.

# Firstly, if we so desire, let us clean up our workspace.
rm(list = ls())

# Next, we must import the packages that we will need to undertake our 
# work with ANN's. These will be the same as last week as we still need 
# 'neuralnet' for some utilities. We add the 'MASS' package for this dataset 
# which will be described below.
if (!require("pacman")) install.packages("pacman")
pacman::p_load("dplyr", "caret", "nnet", "neuralnet", "NeuralNetTools", "MASS")

##########################################################
# PART ONE: Basic regression Neural Network with 'nnet'. #
##########################################################

# It is safe to say from our experiences last week, 'nnet' is easier to use 
# than 'neuralnet', however we can use either. This week, we will be looking 
# at a regression problem - house prices in Boston using the 'MASS' dataset 
# built in to R. We will load this in the same way we did with the iris dataset,
# and then explore it.
house_df <- Boston
summary(house_df)

# For more information on each of the columns, read the R Documentation at the 
# following address: https://www.rdocumentation.org/packages/MASS/versions/
# 7.3-54/topics/Boston. Yes, that median value really is in thousands!

# Do any changes need to be made to the datatypes within this dataset? If so, 
# make them below this comment. (HINT: there is one, recall using as.factor).

# I disagree with the racial component being included in this dataset, so let's 
# remove it now using 'dplyr'.
house_df <- house_df %>% dplyr::select(-black)

# It is a good idea to normalise our data before training the neural network. 
# Depending on the dataset, this could save us a bunch of time and/or the 
# ability to generate meaningful results at all - the model may not converge. 
# We will scale between [0, 1] using the code below.
normalise_func <- function(x) { (x - min(x)) / (max(x) - min(x))}
house_scaled_df <- house_df %>% mutate(across(where(is.numeric), 
  normalise_func))

# We then need to split our dataset, as we almost always do with our machine 
# learning algorithms, into our training and test datasets. This remains 
# unchanged from our use of 'createDataPartition'. We want to use our 'medv' 
# (median house value) instead.
train_idx <- createDataPartition(house_scaled_df$medv, p = 0.8, list = FALSE)
house_train_df <- house_scaled_df[train_idx, ]
house_test_df <- house_scaled_df[-train_idx, ]

# We can now run our model using 'caret' and 'nnet'.
house_net <- train(medv ~ ., house_train_df, method = 'nnet', linout = TRUE, 
  trace = FALSE)

# Let us look at some information from our training. Your tutor will explain 
# what you are seeing.
print(house_net)

# We can then predict over the test set, as we did before.
house_pred <- predict(house_net, house_test_df)

# Our results must differ due to the fact we are predicting something 
# continuous. Hence, we can look at a standard plot of predicted vs actual. But
# first, we need to generate a DataFrame showing this.
house_pa_df <- cbind(as.data.frame(house_pred), house_test_df$medv)
colnames(house_pa_df) <- c("Predicted", "Actual")

# Then, let us plot an actual graph of the predictions (with a 45 degree line 
# that indicates prediction = actual).
plot(x = house_pa_df$Predicted, y = house_pa_df$Actual) + abline(0, 1)

# We can still look at the actual plot of the neural network.
plotnet(house_net)

##########################################################
# PART TWO: Hyperparameter tuning with 'Boston' dataset. #
##########################################################

# The above model seems to do a decent job, however we may be able to get a 
# bettter result with a different number of hidden neurons (after all, 
# everyone disagrees on a good number) and/or number of hidden layers (likely 
# not) as well as the decay function used as part of learning with 
# backpropagation. We can adjust these using Caret. We can see above this was 
# done automatically for us in the 'nnet' command. However, it is a bit more 
# complex if we are doing it ourselves using a 'grid' of these option 
# combinations. We can define a tree as follows, where each array defines the 
# possible element values.
train_grid <- expand.grid(size = c(1, 3, 5, 7, 9), decay = c(0, 1e-4, 1e-3, 
  1e-2, 1e-1))

# We can then feed this into our 'train' command to use these as our parameter 
# options, generating a network for each and then selecting the network with 
# the smallest root mean square error (RMSE).
house_tune_net <- train(medv ~ ., house_train_df, method = 'nnet', 
  linout = TRUE, trace = FALSE, tuneGrid = train_grid, metric = "RMSE",
  maxit = 100)

# Let us see the results of doing this. Does this explain the error message 
# you saw above? Change 'trace' to true, and it may reveal itself. Change 
# 'maxit' to 9000 to see what this does as well. This will take a while, but 
# should only affect those networks that need the extra iterations to converge 
# (hint hint)!
print(house_tune_net)

# Depending on the random starting points, you may get a different answer, 
# however the answers favour mid to larger values for both parameters (generally
# a decay of 0.01 and five or more neurons). Let us predict and plot as above.
house_tune_pred <- predict(house_tune_net, house_test_df)
house_tune_pa_df <- cbind(as.data.frame(house_tune_pred), house_test_df$medv)
colnames(house_tune_pa_df) <- c("Predicted", "Actual")
plot(x = house_tune_pa_df$Predicted, y = house_tune_pa_df$Actual) + abline(0, 1)

# Let us look at the neural network itself. Was it really worth it? Compare the 
# RMSE values for the chosen networks as well as the visuals.
plotnet(house_tune_net)

# Fun fact, you can also do decision trees with 'caret' and 'train'. Do you 
# recall similarities from earlier in semester?

#######################################################
# PART THREE: Cross-validation with 'Boston' dataset. #
#######################################################

# We can now look at using cross-validation. This is similar to the process of 
# bagging that we use with decision trees. However, it is not really helpful 
# except as an additional component to hyperparameter tuning. Hence, you will 
# see here that the (default) hyperparameter tuning process is still undertaken.
house_cv_net <- train(medv ~., house_train_df, method = 'nnet', linout = TRUE, 
 trace = TRUE, metric = "RMSE", trControl = trainControl(method = "cv"))

# Let us look at the result. Note the resampling section with cross-validation.
# Ten-fold simply means that the training data was split into ten and then 
# the model fitted with one group as a test set and the other nine as the 
# training sets, ten times.
print(house_cv_net)

# Let us cheat and 'fix' our parameters to see if it offers any improvement 
# in our diagnostic error characteristics compared to our generally 'best' 
# outcome of tuning to five neurons and a decay of 0.01 above.
train_grid_fixed <- expand.grid(size = c(5), decay = c(0.01))
house_cv_net_fixed <- train(medv ~., house_train_df, method = 'nnet', 
  linout = TRUE, trace = FALSE, metric = "RMSE", 
  trControl = trainControl(method = "cv"), tuneGrid = train_grid_fixed)
print(house_cv_net_fixed)

# For comparison, without the CV...
house_cv_net_fixed_nocv <- train(medv ~., house_train_df, method = 'nnet', 
  linout = TRUE, trace = FALSE, metric = "RMSE", tuneGrid = train_grid_fixed)
print(house_cv_net_fixed_nocv)

# If you wish, predict and test on the various models below.

#########################################################################
# PART FOUR: Hyperparameter tuning and cross-validation with bank churn #
#            dataset.                                                   #
#########################################################################

# This is where you come in! Replicate the process above but using the bank
# churn dataset that we used last week (the _ANN.csv version). Compare the 
# results from your differing models. What are the best parameters?

## PUT YOUR CODE HERE ##

########################
# DISCUSSION QUESTIONS #
########################

# There are no separate discussion questions in this practical. Rather, they are
# dispersed throughout the entirety of the practical above.

# If you have any time left over, you may wish to converse with your team 
# regarding the team assignment.

# Next week, we will be looking at revision and blockchains. The last thing the 
# world needs is a blockchain without a purpose, so the primary focus will be 
# revision.

############################
# END OF SEMINAR/PRACTICAL #
############################